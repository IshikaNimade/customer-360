from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.utils.dates import timedelta
from airflow.sensors.http_sensor import HttpSensor
from airflow.operators.bash_operator import BashOperator
from airflow.contrib.hooks.ssh_hook import SSHHook
from airflow.contrib.operators.ssh_operator import SSHOperator
from airflow.operators.dummy_operator import DummyOperator
#
from airflow import settings
from airflow.models import Connection
#from airflow.contrib.hooks.sqoop_hook
#from airflow.contrib.operators.datastore_import_operator
#from airflow.contrib.operators.vertica_to_mysql
#from airflow.models
#from airflow.models.connection
##
from airflow.contrib.operators.slack_webhook_operator import SlackWebhookOperator


dag = DAG(
	dag_id = 'Customer_360_Pipline_College',
	start_date = days_ago(1)
	)

sensor = HttpSensor(
	task_id = 'watch_for_orders_from_S3',
	http_conn_id = 'order_s3',
	endpoint = 'orders.csv',
	response_check = lambda response: response.status_code == 200,
	dag = dag,
	retry_delay = timedelta(minutes=5),
	retries = 12
)

def get_order_url():
	session =  settings.Session()
	connection = session.query(Connection).filter(Connection.conn_id == 'order_s3').first()
	return f'{connection.schema}://{connection.host}/orders.csv'

#download_order_cmd = f'rm -rf airflow_pipeline && mkdir -p airflow_pipeline && cd airflow_pipeline && wget {get_order_url()}'
download_order_cmd = f'rm -rf airflow_pipeline && mkdir -p airflow_pipeline && cd airflow_pipeline && wget {get_order_url()} && sed -i 1d orders.csv'

download_to_edgenode = SSHOperator(
	task_id = 'download_orders',
	ssh_conn_id = 'cloudera', 
	command = download_order_cmd,  
	dag = dag
)


def fetch_customer_info_cmd():
	command_one = "hive -e 'DROP TABLE airflow.customers'"
	command_one_ext = 'hdfs dfs -rm -R  /user/hive/warehouse/airflow.db/'
	command_two = "sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table customers --warehouse-dir /user/hive/warehouse --hive-import --create-hive-table --hive-table airflow.customers --fields-terminated-by \',\'"
	command_three = "exit 0"
	#return f'{command_one} && {command_one_ext} && ({command_two} || {command_three})'
	return f'({command_two} || {command_three})'



import_cust_to_hive_txns = SSHOperator(
 task_id='sqoop_import_cust_to_hive_txns',
 ssh_conn_id='cloudera',
 command=fetch_customer_info_cmd(),
 dag=dag
)

upload_orders_info = SSHOperator(
	task_id = 'upload_orders_to_hdfs',
	ssh_conn_id = 'cloudera',
	command = 'hdfs dfs -rm -R -f airflow_input && hdfs dfs -mkdir -p airflow_input && hdfs dfs -put ./airflow_pipeline/orders.csv airflow_input/',
	#command = 'hdfs dfs -rm -R -f airflow_input && hdfs dfs -mkdir -p airflow_input && hdfs dfs -put ./airflow_pipeline/orders.csv airflow_input/ && sed -i 1d airflow_input/#orders.csv',
	dag = dag
)


def get_order_filter_cmd():
	command_zero = 'export SPARK_MAJOR_VERSION=2'
	command_one = 'hdfs dfs -rm -R -f airflow_output'
	#command_two = 'spark-submit --class Filter_Orders Airflow_bundell.jar airflow_input/orders.csv airflow_output'
	#command_two = 'spark-submit --class Filter_Orders Airflow_bundel_Sub2.jar airflow_input/orders.csv airflow_output'  Filter_S3_Orders
	#command_two = 'spark-submit --class Filter_S3_Orders Airflow_bundel_Sub26.jar airflow_input/orders.csv  airflow_output/'
	command_two = 'spark-submit --class Filter_S3_Orders Airflow_bundel_Sub27.jar airflow_input/orders.csv'
	return f'{command_zero} && {command_one} && {command_two}'

process_orders_info = SSHOperator(
	task_id = 'process_orders_with_Spark',
	ssh_conn_id = 'cloudera',
	command = get_order_filter_cmd(),
	dag = dag
)


def create_order_hive_table_cmd():
	return 'hive -e "CREATE external table if not exists airflow.orders(order_id string, order_date string, customer_id int, status string) row format delimited fields terminated by \',\' stored as textfile location \'/user/cloudera/airflow_output/\'"'

create_order_table = SSHOperator(
	task_id = 'create_orders_table_hive',
	ssh_conn_id = 'cloudera',
	command = create_order_hive_table_cmd(),
	dag = dag
)


def load_hbase_cmd():
	command_one = 'hive -e "create table airflow.airflow_hbase(customer_id int, customer_fname string, customer_lname string, status string, orders_date string) STORED BY \'org.apache.hadoop.hive.hbase.HBaseStorageHandler\' with SERDEPROPERTIES(\'hbase.columns.mapping\'=\':key,personal:customer_fname, personal:customer_lname, personal:status, personal:order_date\')"'
	command_two = 'hive -e "insert overwrite table airflow.airflow_hbase select c.customer_id, c.customer_fname, c.customer_lname, o.status, o.order_date from airflow.customers c join airflow.orders o ON (c.customer_id = o.customer_id)"'
	return f'{command_one} && {command_two}'

load_hbase = SSHOperator(
	task_id = 'load_hbase_table',
	ssh_conn_id = 'cloudera',
	command = load_hbase_cmd(),
	dag = dag
)

#dummy = DummyOperator(
# task_id='dummy',
# dag = dag
#)


#def slack_password():
#	return '/TU2LPJ0LS/B022JRSQT7E/RXcajVe4t9wAPPFB78ZQMvkm'

def slack_password():
	return '/T0223P2UTV4/B021YA8KE4X/gLEAqn16VcTdWOqOYxIyeMhP'

success_notify = SlackWebhookOperator(
	task_id = 'success_notify',
	http_conn_id = 'slack_webhook',
	message = 'Data loaded successfully in HBase :elephant:',
	channel = '#customer-360-notification',
	username = 'Customer_360',
	icon_emoji = 'elephant',
	webhook_token = slack_password(),
	dag = dag
)

failure_notify = SlackWebhookOperator(
	task_id = 'failure_notify',
	http_conn_id = 'slack_webhook',
	message = 'Data loaded failure for HBase :disguised_face:',
	channel = '#customer-360-notification',
	#channel = '#bigdata-hadoop-spark-developer',
	username = 'airflow',
	icon_emoji = 'disguised_face',
	webhook_token = slack_password(),
	dag = dag,
	trigger_rule = 'all_failed'
)

sensor >> import_cust_to_hive_txns #>> dummy

sensor >> download_to_edgenode >> upload_orders_info >> process_orders_info >> create_order_table #>> load_hbase >> dummy

[import_cust_to_hive_txns , create_order_table ] >> load_hbase #>> dummy

load_hbase >> [success_notify , failure_notify]


#sensor >> import_cust_to_hive_txns

#sensor >> download_to_edgenode >> upload_orders_info >> process_orders_info >> dummy

#sensor >> download_to_edgenode >> import_cust_to_hive_txns >> dummy



NOTE:->
[0]
mysql -u root -p

beeline -u jdbc:hive2://

[1] {TAB}
gedit .bashrc
source .bashrc

[2] 
drop table airflow_hbase;
drop table orders;
drop table customers;

(select * from airflow_hbase  limit 10;)

[3]
hadoop fs -rm -r /user/hive/warehouse/airflow.db

[4]
--Run in command shell
service --status-all
sudo service hbase-master restart
sudo service hbase-regionserver restart

[5]
--Run in HBase Shell
disable 'card_transactions'
drop 'card_transactions'

[6]
https://customer360pi-wa48504.slack.com/archives/C021VPL20DD

https://trend tech-bigdata.s3.ap-south-1.amazonaws.com/orders.csv



[i]
//remove blank lines
sed -i '/^\s*$/d' file_20160802.csv

//remove header
sed -i 1d file_20160802.csv

[ii]
mount -t vboxsf Shared Shared
cd Shared
[iii]
mv /home/cloudera/Desktop/Shared/Airflow_bundel_Sub27.jar /home/cloudera


C:\Users\ASUS\Documents\Apowersoft\Apowersoft Free Screen Recorder